## 动漫屋爬取分析

#### 思路：

确定目标网站 -> 踩点网站(数据在哪) -> 分析api参数 -> js逆向破解 -> 伪造请求 -> 获取数据/保存

#### 详解

1. 漫画大全 http://www.dm5.com/manhua-list/ 

2. 踩点看数据

   漫画目录数据json：（可以源码找到，也有XHR api可以获取到）

   标题 、链接、最新、作者

   下载单本漫画：./manhua/mhName/chapter/pics

   - 一章在一页中（源码有数据，XHR没有）：

     直接源码获取图片链接

   - 每一话有多页（源码无数据，XHR有可疑eval函数js文本数据）：

     console执行js文本，可得图片链接列表，本页图片链接就是第一个元素。

     

3. 找的数据后开始查看分析该api参数：只需分析每话有多页情况

   api: 'http://www.dm5.com/m1058716/chapterfun.ashx?cid=1058716&page=1&key=&language=1&gtk=6&_cid=1058716&_mid=61292&_dt=2020-10-11+23%3A41%3A56&_sign=57dbaa12348a5f27cfd6907535c52958'

   全局搜索大法：

   > data: {
   >     cid: DM5_CID, # 源代码
   >     page: DM5_PAGE, #总页数：DM5_IMAGE_COUNT
   >     key: mkey, #None
   >     language: 1,
   >     gtk: 6,
   >     _cid: DM5_CID, #
   >     _mid: DM5_MID, #
   >     _dt: DM5_VIEWSIGN_DT, #re匹配时注意不要把双引号匹配进去
   >     _sign: DM5_VIEWSIGN #
   > }

   可变参数：page

   其他参数都可以通过章节源码获取（_dt, _sign看起来可疑，但翻页发现其没有变化，说明可以在原码获取该参数）

4. 伪造api请求（添加referer请求头，不然没有数据返回），获取js代码片段

5. 执行js（execjs.eval(response.text)），获取图片链接列表

6. 图片下载与保存（创建存储目录，图片名通过urlparse(img_url)获取）

   

   

   

笔记:

0. 创建多级目录，可用

   ```pyhton
path_1 = os.sep.join(['漫画','漫画名'])
   if not exists(path_1):
   	makedirs(path_1)
   ```

1. api参数_dt, _sign看起来可疑，但翻页发现其没有变化，说明可以在原码获取该参数

2. 漫画列表反爬：请求拦截器（axios）

   从api响应获取漫画列表数据，响应码200，但是得不到json数据，原因可能是被请求拦截器拦截了

   解决：查看api是否正确（key参数没有值，设为None不要设为0），添加referer请求头（不要可变的，只要host）

3. 单章单页情况，用获取的data-src图片链接下载图片，要么下载失败，要么下载到不正确的图“正义”片:

   > http://manhua1034-101-69-161-99.cdndm5.com/63/62099/1063579/1_6818.jpg?cid=1063579&key=8e3c35912ce2817a2bfa42cac06df1cf&type=1
   > 保存1_6818.jpg成功！
   > http://manhua1034-101-69-161-99.cdndm5.com/63/62099/1063579/2_9753.jpg?cid=1063579&key=8e3c35912ce2817a2bfa42cac06df1cf&type=1
   > 保存2_9753.jpg成功！
   > http://manhua1034-101-69-161-99.cdndm5.com/63/62099/1063579/3_4950.jpg?cid=1063579&key=8e3c35912ce2817a2bfa42cac06df1cf&type=1
   > 保存3_4950.jpg成功！
   
   解决：下载失败加个try，可能是网络问题
   
4. 下载单章多页下载到某一页时，报错：

   > img_name = urlparse(img).path.split('/')[-1]
   >
   > TypeError: a bytes-like object is required, not 'str'

   解决：不管它，重新请求一次，成功下载